{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bank_df = pd.read_csv('bank.csv', delimiter=';')\n",
    "\n",
    "file_path = 'adult.data'\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "    'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "adult_df = pd.read_csv(file_path, names=columns, na_values=\"?\", skipinitialspace=True)\n",
    "\n",
    "# Binary indicators for capital gains and losses\n",
    "adult_df['positive_capital_gain'] = (adult_df['capital-gain'] > 0).astype(int)\n",
    "adult_df['positive_capital_loss'] = (adult_df['capital-loss'] > 0).astype(int)\n",
    "\n",
    "# Interaction feature between age and education-num\n",
    "adult_df['age_education_interaction'] = adult_df['age'] * adult_df['education-num']\n",
    "\n",
    "# Aggregating less common categories into 'Other'\n",
    "top_occupations = adult_df['occupation'].value_counts().nlargest(5).index\n",
    "adult_df['occupation_aggregated'] = adult_df['occupation'].apply(lambda x: x if x in top_occupations else 'Other')\n",
    "\n",
    "top_countries = adult_df['native-country'].value_counts().nlargest(5).index\n",
    "adult_df['native_country_aggregated'] = adult_df['native-country'].apply(lambda x: x if x in top_countries else 'Other')\n",
    "\n",
    "# Binning age and hours-per-week\n",
    "age_bins = [0, 25, 35, 45, 55, 65, 100]\n",
    "age_labels = ['0-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "adult_df['age_binned'] = pd.cut(adult_df['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "hours_bins = [0, 20, 30, 40, 50, 100]\n",
    "hours_labels = ['0-20', '21-30', '31-40', '41-50', '51+']\n",
    "adult_df['hours_per_week_binned'] = pd.cut(adult_df['hours-per-week'], bins=hours_bins, labels=hours_labels, right=False)\n",
    "\n",
    "\n",
    "# Aggregating less common categories into 'Other'\n",
    "top_jobs = bank_df['job'].value_counts().nlargest(5).index\n",
    "bank_df['job_aggregated'] = bank_df['job'].apply(lambda x: x if x in top_jobs else 'Other')\n",
    "\n",
    "# Binning age and hours-per-week\n",
    "age_bins = [0, 25, 35, 45, 55, 65, 75, 100]\n",
    "age_labels = ['0-25', '26-35', '36-45', '46-55', '56-65', '66-75', '75+']\n",
    "bank_df['age_binned'] = pd.cut(bank_df['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "bank_df = bank_df.drop(['age', 'job'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "### Preprocessing Function ###\n",
    "def preprocess(df):\n",
    "    df_clean = df.dropna()\n",
    "    cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "    df_clean = pd.get_dummies(df_clean, columns=cat_cols, drop_first=True)\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    df_clean[num_cols] = scaler.fit_transform(df_clean[num_cols])\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transaction Conversion Function ###\n",
    "def dataframe_to_transactions(df):\n",
    "    transactions = []\n",
    "    for _, row in df.iterrows():\n",
    "        transaction = [col for col in df.columns if row[col] == 1]\n",
    "        transactions.append(transaction)\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dbscan_params(df, eps_values, min_samples_values):\n",
    "    best_score = -1\n",
    "    best_params = {'eps': None, 'min_samples': None}\n",
    "    best_labels = None\n",
    "    best_cluster_count = 0\n",
    "    total_points = df.shape[0]\n",
    "    max_noise_points = total_points * 0.10  # 10% of total points as maximum allowed noise\n",
    "\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(df)\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            noise_points = np.count_nonzero(labels == -1)\n",
    "\n",
    "            if n_clusters > 1 and noise_points <= max_noise_points:\n",
    "                if -1 in labels:\n",
    "                    silhouette = silhouette_score(df[labels != -1], labels[labels != -1])\n",
    "                else:\n",
    "                    silhouette = silhouette_score(df, labels)\n",
    "                \n",
    "                # Score adjusted to prioritize configurations with less noise\n",
    "                score = 2 * silhouette + np.log1p(n_clusters) - np.log1p(noise_points)\n",
    "                print(f\"Testing eps={eps}, min_samples={min_samples}: silhouette={silhouette:.4f}, clusters={n_clusters}, noise={noise_points}\")\n",
    "\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "                    best_labels = labels.copy()\n",
    "                    best_cluster_count = n_clusters\n",
    "\n",
    "    if best_params['eps']:\n",
    "        print(f\"Best Parameters: eps={best_params['eps']}, min_samples={best_params['min_samples']}, score={best_score:.4f}\")\n",
    "        print(f\"Clusters found: {best_cluster_count}, Noise: {np.count_nonzero(best_labels == -1)}\")\n",
    "    else:\n",
    "        print(\"No valid clustering configuration found.\")\n",
    "        best_labels = []\n",
    "\n",
    "    return best_params, best_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Clusters & FP-GROWTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "### Cluster Analysis Function ###\n",
    "def analyze_clusters(df, labels):\n",
    "    df['Cluster'] = labels  \n",
    "    # Visualizing the distribution of clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=labels, palette='viridis')\n",
    "    plt.title('Distribution of Clusters')\n",
    "    plt.xlabel('Cluster Label')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.show()\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "### FP-Growth Association Rule Mining Function ###\n",
    "def apply_fp_growth(transactions, min_support=0.1, min_confidence=0.3, min_lift=3):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    frequent_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules = rules[rules['lift'] >= min_lift]\n",
    "    return rules\n",
    "\n",
    "### Rule Comparison and Filtering Function ###\n",
    "def filter_redundant_rules(global_rules, cluster_rules):\n",
    "    cluster_rules_filtered = cluster_rules[~cluster_rules['antecedents'].isin(global_rules['antecedents']) & ~cluster_rules['consequents'].isin(global_rules['consequents'])]\n",
    "    return cluster_rules_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_clusters(df, labels):\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(df.drop(['Cluster'], axis=1))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(principal_components[:, 0], principal_components[:, 1], c=labels, cmap='viridis', label=labels)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('2D PCA of Dataset')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "def opticsPlot(df, min_samples):\n",
    "    optics_model = OPTICS(min_samples=min_samples, xi=0.05, min_cluster_size=0.05)\n",
    "    optics_model.fit(df)\n",
    "    reachability = optics_model.reachability_[optics_model.ordering_]\n",
    "    labels = optics_model.labels_[optics_model.ordering_]\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.bar(range(len(reachability)), reachability)\n",
    "    plt.title('Reachability Plot')\n",
    "    plt.xlabel('Ordered Points')\n",
    "    plt.ylabel('Reachability Distance')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_clustering_and_rule_mining(df, eps_values, min_samples_values, min_support, min_confidence, min_lift):\n",
    "    processed_df = preprocess(df)\n",
    "    best_params, best_labels = estimate_dbscan_params(processed_df, eps_values, min_samples_values)\n",
    "\n",
    "    processed_df['Cluster'] = best_labels\n",
    "\n",
    "    # Plotting the clusters\n",
    "    plot_clusters(processed_df, processed_df['Cluster'])\n",
    "    opticsPlot(processed_df, best_params['min_samples'])\n",
    "    # Check if all labels are -1 (indicating all data points are considered noise)\n",
    "    if (best_labels == -1).all():\n",
    "        print(\"No valid clusters formed, all points are labeled as noise.\")\n",
    "        return {}, {}  # Return empty dictionaries if no valid clusters are formed\n",
    "\n",
    "    analyze_clusters(processed_df, best_labels)\n",
    "    \n",
    "    transactions = dataframe_to_transactions(processed_df)\n",
    "    initial_rules = apply_fp_growth(transactions, min_support, min_confidence, min_lift)\n",
    "    \n",
    "    cluster_specific_rules = {}\n",
    "    for cluster_label in set(best_labels):\n",
    "        if cluster_label == -1:  # Skip the noise cluster\n",
    "            continue\n",
    "        cluster_transactions = dataframe_to_transactions(processed_df[best_labels == cluster_label])\n",
    "        cluster_rules = apply_fp_growth(cluster_transactions, min_support, min_confidence, min_lift)\n",
    "        filtered_rules = filter_redundant_rules(initial_rules, cluster_rules)\n",
    "        cluster_specific_rules[cluster_label] = filtered_rules\n",
    "    \n",
    "    return initial_rules, cluster_specific_rules, best_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter range for DBSCAN\n",
    "#eps_values = np.linspace(3, 5, 5)\n",
    "#min_samples_values = range(110, 150, 4)  \n",
    "\n",
    "eps_values = np.linspace(1, 2, 2)\n",
    "min_samples_values = range(5, 10, 5)  \n",
    "\n",
    "# Define thresholds for FP-Growth\n",
    "min_support = 0.1\n",
    "min_confidence = 0.3\n",
    "min_lift = 3\n",
    "\n",
    "# Run the pipeline\n",
    "initial_rules, cluster_specific_rules, best_labels = automated_clustering_and_rule_mining(\n",
    "    bank_df, eps_values, min_samples_values, min_support, min_confidence, min_lift\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Initial Rules Found:\")\n",
    "print(initial_rules)\n",
    "\n",
    "print(\"\\nCluster Specific Rules:\")\n",
    "for cluster, rules in cluster_specific_rules.items():\n",
    "    print(f\"Rules for Cluster {cluster}:\")\n",
    "    print(rules)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_k_distance_graph(data, k):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(data)\n",
    "    distances, indices = neigh.kneighbors(data)\n",
    "    \n",
    "    sorted_distances = np.sort(distances[:, k-1], axis=0)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sorted_distances)\n",
    "    plt.title(\"K-distance Graph\")\n",
    "    plt.xlabel(\"Points sorted by distance\")\n",
    "    plt.ylabel(f\"Distance to {k}-th nearest neighbor\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "processed_data = preprocess(adult_df)  # Assuming preprocess is your function to standardize and encode the data\n",
    "plot_k_distance_graph(processed_data, 43)  # k can be set to the min_samples you are considering\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
