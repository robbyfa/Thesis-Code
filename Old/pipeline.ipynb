{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bank_df = pd.read_csv('bank.csv', delimiter=';')\n",
    "\n",
    "file_path = 'adult.data'\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
    "    'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "adult_df = pd.read_csv(file_path, names=columns, na_values=\"?\", skipinitialspace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary indicators for capital gains and losses\n",
    "adult_df['positive_capital_gain'] = (adult_df['capital-gain'] > 0).astype(int)\n",
    "adult_df['positive_capital_loss'] = (adult_df['capital-loss'] > 0).astype(int)\n",
    "\n",
    "# Interaction feature between age and education-num\n",
    "adult_df['age_education_interaction'] = adult_df['age'] * adult_df['education-num']\n",
    "\n",
    "# Aggregating less common categories into 'Other'\n",
    "top_occupations = adult_df['occupation'].value_counts().nlargest(5).index\n",
    "adult_df['occupation_aggregated'] = adult_df['occupation'].apply(lambda x: x if x in top_occupations else 'Other')\n",
    "\n",
    "top_countries = adult_df['native-country'].value_counts().nlargest(5).index\n",
    "adult_df['native_country_aggregated'] = adult_df['native-country'].apply(lambda x: x if x in top_countries else 'Other')\n",
    "\n",
    "# Binning age and hours-per-week\n",
    "age_bins = [0, 25, 35, 45, 55, 65, 100]\n",
    "age_labels = ['0-25', '26-35', '36-45', '46-55', '56-65', '66+']\n",
    "adult_df['age_binned'] = pd.cut(adult_df['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "hours_bins = [0, 20, 30, 40, 50, 100]\n",
    "hours_labels = ['0-20', '21-30', '31-40', '41-50', '51+']\n",
    "adult_df['hours_per_week_binned'] = pd.cut(adult_df['hours-per-week'], bins=hours_bins, labels=hours_labels, right=False)\n",
    "\n",
    "\n",
    "adult_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "### Preprocessing Function ###\n",
    "def preprocess(df):\n",
    "    df = df.dropna()\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    df = pd.get_dummies(df, columns=cat_cols)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transaction Conversion Function ###\n",
    "def dataframe_to_transactions(df):\n",
    "    transactions = []\n",
    "    for _, row in df.iterrows():\n",
    "        transaction = []\n",
    "        for col in df.columns:\n",
    "            # This check ensures that only non-zero entries (True, for one-hot encoded columns) are included.\n",
    "            if row[col] == 1:\n",
    "                transaction.append(col)\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing eps=1.5999999999999999, min_samples=2 --> Silhouette: -0.1999, clusters: 161\n",
      "Testing eps=1.5999999999999999, min_samples=3 --> Silhouette: -0.1791, clusters: 50\n",
      "Testing eps=1.7, min_samples=2 --> Silhouette: -0.2021, clusters: 133\n",
      "Testing eps=1.7, min_samples=3 --> Silhouette: -0.1359, clusters: 42\n",
      "Testing eps=1.7, min_samples=4 --> Silhouette: -0.1015, clusters: 19\n",
      "Testing eps=1.7, min_samples=5 --> Silhouette: -0.0585, clusters: 12\n",
      "Testing eps=1.8, min_samples=2 --> Silhouette: -0.2048, clusters: 121\n",
      "Testing eps=1.8, min_samples=3 --> Silhouette: -0.1281, clusters: 36\n",
      "Testing eps=1.8, min_samples=4 --> Silhouette: -0.1058, clusters: 22\n",
      "Testing eps=1.8, min_samples=5 --> Silhouette: -0.0266, clusters: 16\n",
      "Testing eps=1.8, min_samples=6 --> Silhouette: 0.0852, clusters: 7\n",
      "Testing eps=1.8, min_samples=7 --> Silhouette: 0.2585, clusters: 4\n",
      "Testing eps=1.8, min_samples=8 --> Silhouette: 0.0932, clusters: 4\n",
      "Testing eps=1.8, min_samples=9 --> Silhouette: 0.3091, clusters: 3\n",
      "Testing eps=1.8, min_samples=10 --> Silhouette: 0.0525, clusters: 4\n",
      "Testing eps=1.9, min_samples=2 --> Silhouette: -0.1723, clusters: 90\n",
      "Testing eps=1.9, min_samples=3 --> Silhouette: -0.0866, clusters: 24\n",
      "Testing eps=1.9, min_samples=4 --> Silhouette: -0.0690, clusters: 21\n",
      "Testing eps=1.9, min_samples=5 --> Silhouette: -0.0337, clusters: 15\n",
      "Testing eps=1.9, min_samples=6 --> Silhouette: -0.0116, clusters: 10\n",
      "Testing eps=1.9, min_samples=7 --> Silhouette: 0.0610, clusters: 8\n",
      "Testing eps=1.9, min_samples=8 --> Silhouette: 0.1171, clusters: 5\n",
      "Testing eps=1.9, min_samples=9 --> Silhouette: 0.2973, clusters: 3\n",
      "Testing eps=1.9, min_samples=10 --> Silhouette: 0.2977, clusters: 3\n",
      "Testing eps=1.9, min_samples=11 --> Silhouette: 0.2977, clusters: 3\n",
      "Testing eps=1.9, min_samples=12 --> Silhouette: 0.3035, clusters: 3\n",
      "Testing eps=1.9, min_samples=13 --> Silhouette: 0.3041, clusters: 3\n",
      "Testing eps=1.9, min_samples=14 --> Silhouette: 0.3050, clusters: 3\n",
      "Testing eps=1.9, min_samples=15 --> Silhouette: 0.3062, clusters: 3\n",
      "Testing eps=1.9, min_samples=16 --> Silhouette: 0.3159, clusters: 3\n",
      "Testing eps=2.0, min_samples=2 --> Silhouette: -0.1644, clusters: 73\n",
      "Testing eps=2.0, min_samples=3 --> Silhouette: -0.0647, clusters: 18\n",
      "Testing eps=2.0, min_samples=4 --> Silhouette: -0.0329, clusters: 12\n",
      "Testing eps=2.0, min_samples=5 --> Silhouette: -0.0220, clusters: 10\n",
      "Testing eps=2.0, min_samples=6 --> Silhouette: 0.0029, clusters: 8\n",
      "Testing eps=2.0, min_samples=7 --> Silhouette: 0.0024, clusters: 8\n",
      "Testing eps=2.0, min_samples=8 --> Silhouette: 0.0435, clusters: 8\n",
      "Testing eps=2.0, min_samples=9 --> Silhouette: 0.1383, clusters: 5\n",
      "Testing eps=2.0, min_samples=10 --> Silhouette: 0.1261, clusters: 5\n",
      "Testing eps=2.0, min_samples=11 --> Silhouette: 0.2934, clusters: 3\n",
      "Testing eps=2.0, min_samples=12 --> Silhouette: 0.2925, clusters: 3\n",
      "Testing eps=2.0, min_samples=13 --> Silhouette: 0.2935, clusters: 3\n",
      "Testing eps=2.0, min_samples=14 --> Silhouette: 0.2967, clusters: 3\n",
      "Testing eps=2.0, min_samples=15 --> Silhouette: 0.3013, clusters: 3\n",
      "Testing eps=2.0, min_samples=16 --> Silhouette: 0.3021, clusters: 3\n",
      "Testing eps=2.0, min_samples=17 --> Silhouette: 0.3026, clusters: 3\n",
      "Testing eps=2.0, min_samples=18 --> Silhouette: 0.3043, clusters: 3\n",
      "Testing eps=2.0, min_samples=19 --> Silhouette: 0.3011, clusters: 3\n",
      "Best Parameters: eps=1.9, min_samples=16 with silhouette=0.3159\n",
      "({'eps': 1.9, 'min_samples': 16}, array([-1, -1, -1, ...,  0, -1, -1]))\n"
     ]
    }
   ],
   "source": [
    "def estimate_dbscan_params(df, eps_values, min_samples_values):\n",
    "    best_silhouette = -1\n",
    "    best_params = None\n",
    "    best_labels = None\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(df)\n",
    "            \n",
    "            if len(set(labels)) < 2 or np.count_nonzero(labels == -1) > 0.5 * len(labels):\n",
    "                continue\n",
    "            \n",
    "            silhouette = silhouette_score(df[labels != -1], labels[labels != -1]) if -1 in labels else silhouette_score(df, labels)\n",
    "            \n",
    "            print(f\"Testing eps={eps}, min_samples={min_samples} --> Silhouette: {silhouette:.4f}, clusters: {len(set(labels))}\")\n",
    "\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "                best_labels = labels\n",
    "                \n",
    "    print(f\"Best Parameters: eps={best_params['eps']}, min_samples={best_params['min_samples']} with silhouette={best_silhouette:.4f}\")\n",
    "    return best_params, best_labels\n",
    "\n",
    "preprocessed_df = preprocess(bank_df)\n",
    "eps_values=np.linspace(0.1, 2.0, 20)\n",
    "min_samples_values=range(2, 20)\n",
    "\n",
    "print(estimate_dbscan_params(preprocessed_df, eps_values, min_samples_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FP-Growth Association Rule Mining Function ###\n",
    "def apply_fp_growth(transactions, min_support=0.1, min_confidence=0.3, min_lift=3):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    frequent_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules = rules[rules['lift'] >= min_lift]\n",
    "    \n",
    "    return rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "### Preprocessing Function ###\n",
    "def preprocess(df):\n",
    "    df_clean = df.dropna()\n",
    "    cat_cols = df_clean.select_dtypes(include=['object', 'category']).columns\n",
    "    df_clean = pd.get_dummies(df_clean, columns=cat_cols)\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    df_clean[num_cols] = scaler.fit_transform(df_clean[num_cols])\n",
    "    return df_clean\n",
    "\n",
    "### Transaction Conversion Function ###\n",
    "def dataframe_to_transactions(df):\n",
    "    transactions = []\n",
    "    for _, row in df.iterrows():\n",
    "        transaction = [col for col in df.columns if row[col] == 1]\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "### DBSCAN with Parameter Estimation Function ###\n",
    "def estimate_dbscan_params(df, eps_values, min_samples_values):\n",
    "    best_silhouette = -1\n",
    "    best_params = None\n",
    "    best_labels = None\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(df)\n",
    "            if len(set(labels)) < 3 or np.count_nonzero(labels == -1) > 0.5 * len(labels):\n",
    "                continue\n",
    "            if -1 in labels:\n",
    "                silhouette = silhouette_score(df[labels != -1], labels[labels != -1])\n",
    "            else:\n",
    "                silhouette = silhouette_score(df, labels)\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "                best_labels = labels\n",
    "\n",
    "    return best_params, best_labels\n",
    "\n",
    "### FP-Growth Association Rule Mining Function ###\n",
    "def apply_fp_growth(transactions, min_support=0.1, min_confidence=0.3, min_lift=3):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    frequent_itemsets = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    rules = rules[rules['lift'] >= min_lift]\n",
    "    return rules\n",
    "\n",
    "### Rule Comparison and Filtering Function ###\n",
    "def filter_redundant_rules(global_rules, cluster_rules):\n",
    "    # Normalizing rule itemsets for accurate comparison\n",
    "    global_rules['antecedents'] = global_rules['antecedents'].apply(lambda x: frozenset(x))\n",
    "    global_rules['consequents'] = global_rules['consequents'].apply(lambda x: frozenset(x))\n",
    "    cluster_rules['antecedents'] = cluster_rules['antecedents'].apply(lambda x: frozenset(x))\n",
    "    cluster_rules['consequents'] = cluster_rules['consequents'].apply(lambda x: frozenset(x))\n",
    "\n",
    "    # Filtering out redundant rules\n",
    "    merged = cluster_rules.merge(global_rules, on=['antecedents', 'consequents'], how='left', indicator=True)\n",
    "    unique_cluster_rules = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "    return unique_cluster_rules\n",
    "\n",
    "### Main Automated Pipeline Function ###\n",
    "def automated_clustering_and_rule_mining(df, eps_values, min_samples_values, min_support, min_confidence, min_lift):\n",
    "    processed_df = preprocess(df)\n",
    "    best_params, best_labels = estimate_dbscan_params(processed_df, eps_values, min_samples_values)\n",
    "    transactions = dataframe_to_transactions(processed_df)\n",
    "    initial_rules = apply_fp_growth(transactions, min_support, min_confidence, min_lift)\n",
    "    \n",
    "    cluster_specific_rules = {}\n",
    "    for cluster_label in set(best_labels):\n",
    "        if cluster_label == -1:\n",
    "            continue\n",
    "        cluster_transactions = dataframe_to_transactions(processed_df[best_labels == cluster_label])\n",
    "        cluster_rules = apply_fp_growth(cluster_transactions, min_support, min_confidence, min_lift)\n",
    "        filtered_rules = filter_redundant_rules(initial_rules, cluster_rules)\n",
    "        cluster_specific_rules[cluster_label] = filtered_rules\n",
    "    \n",
    "    return initial_rules, cluster_specific_rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "initial_rules, cluster_specific_rules = automated_clustering_and_rule_mining(\n",
    "    adult_df, \n",
    "    eps_values=np.linspace(0.1, 2.0, 20), \n",
    "    min_samples_values=range(2, 20),\n",
    "    min_support=0.05, \n",
    "    min_confidence=0.2, \n",
    "    min_lift=1.5\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "initial_rules_bank, cluster_specific_rules_bank = automated_clustering_and_rule_mining(\n",
    "    bank_df, \n",
    "    eps_values=np.linspace(0.1, 2.0, 20), \n",
    "    min_samples_values=range(2, 20),\n",
    "    min_support=0.05, \n",
    "    min_confidence=0.2, \n",
    "    min_lift=1.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>(job_blue-collar)</td>\n",
       "      <td>(education_primary)</td>\n",
       "      <td>0.209246</td>\n",
       "      <td>0.149967</td>\n",
       "      <td>0.081619</td>\n",
       "      <td>0.390063</td>\n",
       "      <td>2.600998</td>\n",
       "      <td>0.050239</td>\n",
       "      <td>1.393642</td>\n",
       "      <td>0.778412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>(education_primary)</td>\n",
       "      <td>(job_blue-collar)</td>\n",
       "      <td>0.149967</td>\n",
       "      <td>0.209246</td>\n",
       "      <td>0.081619</td>\n",
       "      <td>0.544248</td>\n",
       "      <td>2.600998</td>\n",
       "      <td>0.050239</td>\n",
       "      <td>1.735053</td>\n",
       "      <td>0.724127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>(job_blue-collar, default_no)</td>\n",
       "      <td>(education_primary)</td>\n",
       "      <td>0.206149</td>\n",
       "      <td>0.149967</td>\n",
       "      <td>0.080734</td>\n",
       "      <td>0.391631</td>\n",
       "      <td>2.611450</td>\n",
       "      <td>0.049819</td>\n",
       "      <td>1.397233</td>\n",
       "      <td>0.777313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054</th>\n",
       "      <td>(default_no, education_primary)</td>\n",
       "      <td>(job_blue-collar)</td>\n",
       "      <td>0.147755</td>\n",
       "      <td>0.209246</td>\n",
       "      <td>0.080734</td>\n",
       "      <td>0.546407</td>\n",
       "      <td>2.611318</td>\n",
       "      <td>0.049817</td>\n",
       "      <td>1.743313</td>\n",
       "      <td>0.724031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4055</th>\n",
       "      <td>(job_blue-collar)</td>\n",
       "      <td>(default_no, education_primary)</td>\n",
       "      <td>0.209246</td>\n",
       "      <td>0.147755</td>\n",
       "      <td>0.080734</td>\n",
       "      <td>0.385835</td>\n",
       "      <td>2.611318</td>\n",
       "      <td>0.049817</td>\n",
       "      <td>1.387649</td>\n",
       "      <td>0.780333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50049</th>\n",
       "      <td>(housing_yes, loan_no, month_jul)</td>\n",
       "      <td>(poutcome_unknown, contact_cellular)</td>\n",
       "      <td>0.060385</td>\n",
       "      <td>0.478655</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.767773</td>\n",
       "      <td>0.022191</td>\n",
       "      <td>3.388741</td>\n",
       "      <td>0.462228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50053</th>\n",
       "      <td>(housing_yes, month_jul)</td>\n",
       "      <td>(poutcome_unknown, loan_no, contact_cellular)</td>\n",
       "      <td>0.076311</td>\n",
       "      <td>0.399690</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.669565</td>\n",
       "      <td>1.675210</td>\n",
       "      <td>0.020594</td>\n",
       "      <td>1.816727</td>\n",
       "      <td>0.436359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50054</th>\n",
       "      <td>(loan_no, month_jul)</td>\n",
       "      <td>(poutcome_unknown, housing_yes, contact_cellular)</td>\n",
       "      <td>0.109710</td>\n",
       "      <td>0.217872</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.465726</td>\n",
       "      <td>2.137611</td>\n",
       "      <td>0.027192</td>\n",
       "      <td>1.463907</td>\n",
       "      <td>0.597769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50056</th>\n",
       "      <td>(month_jul)</td>\n",
       "      <td>(poutcome_unknown, housing_yes, loan_no, conta...</td>\n",
       "      <td>0.156160</td>\n",
       "      <td>0.180933</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.327195</td>\n",
       "      <td>1.808375</td>\n",
       "      <td>0.022840</td>\n",
       "      <td>1.217392</td>\n",
       "      <td>0.529742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50360</th>\n",
       "      <td>(poutcome_unknown, month_nov)</td>\n",
       "      <td>(y_no, contact_cellular)</td>\n",
       "      <td>0.061270</td>\n",
       "      <td>0.548551</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.833935</td>\n",
       "      <td>1.520250</td>\n",
       "      <td>0.017485</td>\n",
       "      <td>2.718507</td>\n",
       "      <td>0.364549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16173 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             antecedents  \\\n",
       "3331                   (job_blue-collar)   \n",
       "3332                 (education_primary)   \n",
       "4052       (job_blue-collar, default_no)   \n",
       "4054     (default_no, education_primary)   \n",
       "4055                   (job_blue-collar)   \n",
       "...                                  ...   \n",
       "50049  (housing_yes, loan_no, month_jul)   \n",
       "50053           (housing_yes, month_jul)   \n",
       "50054               (loan_no, month_jul)   \n",
       "50056                        (month_jul)   \n",
       "50360      (poutcome_unknown, month_nov)   \n",
       "\n",
       "                                             consequents  antecedent support  \\\n",
       "3331                                 (education_primary)            0.209246   \n",
       "3332                                   (job_blue-collar)            0.149967   \n",
       "4052                                 (education_primary)            0.206149   \n",
       "4054                                   (job_blue-collar)            0.147755   \n",
       "4055                     (default_no, education_primary)            0.209246   \n",
       "...                                                  ...                 ...   \n",
       "50049               (poutcome_unknown, contact_cellular)            0.060385   \n",
       "50053      (poutcome_unknown, loan_no, contact_cellular)            0.076311   \n",
       "50054  (poutcome_unknown, housing_yes, contact_cellular)            0.109710   \n",
       "50056  (poutcome_unknown, housing_yes, loan_no, conta...            0.156160   \n",
       "50360                           (y_no, contact_cellular)            0.061270   \n",
       "\n",
       "       consequent support   support  confidence      lift  leverage  \\\n",
       "3331             0.149967  0.081619    0.390063  2.600998  0.050239   \n",
       "3332             0.209246  0.081619    0.544248  2.600998  0.050239   \n",
       "4052             0.149967  0.080734    0.391631  2.611450  0.049819   \n",
       "4054             0.209246  0.080734    0.546407  2.611318  0.049817   \n",
       "4055             0.147755  0.080734    0.385835  2.611318  0.049817   \n",
       "...                   ...       ...         ...       ...       ...   \n",
       "50049            0.478655  0.051095    0.846154  1.767773  0.022191   \n",
       "50053            0.399690  0.051095    0.669565  1.675210  0.020594   \n",
       "50054            0.217872  0.051095    0.465726  2.137611  0.027192   \n",
       "50056            0.180933  0.051095    0.327195  1.808375  0.022840   \n",
       "50360            0.548551  0.051095    0.833935  1.520250  0.017485   \n",
       "\n",
       "       conviction  zhangs_metric  \n",
       "3331     1.393642       0.778412  \n",
       "3332     1.735053       0.724127  \n",
       "4052     1.397233       0.777313  \n",
       "4054     1.743313       0.724031  \n",
       "4055     1.387649       0.780333  \n",
       "...           ...            ...  \n",
       "50049    3.388741       0.462228  \n",
       "50053    1.816727       0.436359  \n",
       "50054    1.463907       0.597769  \n",
       "50056    1.217392       0.529742  \n",
       "50360    2.718507       0.364549  \n",
       "\n",
       "[16173 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_rules_bank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
